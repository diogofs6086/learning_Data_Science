for (i in c("A","B","C","D")) {
train_blc <- ROSE(G3 ~ ., data = data_train[data_train$G3 == 'F' | data_train$G3 == i, ])$data
test_blc <- ROSE(G3 ~ ., data = data_test[data_test$G3 == 'F' | data_test$G3 == i, ])$data
ifelse (c == 0,
rbind(data_train_blc, train_blc),
data_train_blc <- rbind(data_train_blc, train_blc[train_blc$G3 == i, ]))
ifelse (c == 0,
rbind(data_test_blc, test_blc),
data_test_blc <- rbind(data_test_blc, test_blc[test_blc$G3 == i, ]))
}
round(prop.table(table(data_train_blc$G3)), 2)
round(prop.table(table(data_test_blc$G3)), 2)
round(prop.table(table(data_test_blc$G3)), 2)
round(prop.table(table(data_test_blc$G3)), 2)
# "F" is the biggest class, so the balancing will be done by it
data_train_blc <- data.frame()
data_test_blc <- data.frame()
c = 0
for (i in c("A","B","C","D")) {
train_blc <- ROSE(G3 ~ ., data = data_train[data_train$G3 == 'F' | data_train$G3 == i, ])$data
test_blc <- ROSE(G3 ~ ., data = data_test[data_test$G3 == 'F' | data_test$G3 == i, ])$data
ifelse (c == 0,
rbind(data_train_blc, train_blc),
data_train_blc <- rbind(data_train_blc, train_blc[train_blc$G3 == i, ]))
ifelse (c == 0,
rbind(data_test_blc, test_blc),
data_test_blc <- rbind(data_test_blc, test_blc[test_blc$G3 == i, ]))
}
train_blc <- ROSE(G3 ~ ., data = data_train[data_train$G3 == 'F' | data_train$G3 == "A", ])$data
test_blc <- ROSE(G3 ~ ., data = data_test[data_test$G3 == 'F' | data_test$G3 == "A", ])$data
# "F" is the biggest class, so the balancing will be done by it
data_train_blc <- data.frame()
data_test_blc <- data.frame()
c = 0
for (i in c("A","B","C","D")) {
train_blc <- ROSE(G3 ~ ., data = data_train[data_train$G3 == 'F' | data_train$G3 == i, ])$data
test_blc <- ROSE(G3 ~ ., data = data_test[data_test$G3 == 'F' | data_test$G3 == i, ])$data
ifelse (c == 0,
rbind(data_train_blc, train_blc),
data_train_blc <- rbind(data_train_blc, train_blc[train_blc$G3 == i, ]))
ifelse (c == 0,
rbind(data_test_blc, test_blc),
data_test_blc <- rbind(data_test_blc, test_blc[test_blc$G3 == i, ]))
}
round(prop.table(table(data_train_blc$G3)), 2)
round(prop.table(table(data_test_blc$G3)), 2)
ifelse (0 == 0,
rbind(data_test_blc, test_blc),
data_test_blc <- rbind(data_test_blc, test_blc[test_blc$G3 == i, ]))
ifelse (0 == 0,
rbind(data_test_blc, test_blc),
data_test_blc <- rbind(data_test_blc, test_blc[test_blc$G3 == i, ]))
# "F" is the biggest class, so the balancing will be done by it
data_train_blc <- data.frame()
data_test_blc <- data.frame()
c = 0
for (i in c("A","B","C","D")) {
train_blc <- ROSE(G3 ~ ., data = data_train[data_train$G3 == 'F' | data_train$G3 == i, ])$data
test_blc <- ROSE(G3 ~ ., data = data_test[data_test$G3 == 'F' | data_test$G3 == i, ])$data
ifelse (c == 0,
data_train_blc <- rbind(data_train_blc, train_blc),
data_train_blc <- rbind(data_train_blc, train_blc[train_blc$G3 == i, ]))
ifelse (c == 0,
data_test_blc <- rrbind(data_test_blc, test_blc),
data_test_blc <- rbind(data_test_blc, test_blc[test_blc$G3 == i, ]))
c = c +1
}
# "F" is the biggest class, so the balancing will be done by it
data_train_blc <- data.frame()
data_test_blc <- data.frame()
c = 0
for (i in c("A","B","C","D")) {
train_blc <- ROSE(G3 ~ ., data = data_train[data_train$G3 == 'F' | data_train$G3 == i, ])$data
test_blc <- ROSE(G3 ~ ., data = data_test[data_test$G3 == 'F' | data_test$G3 == i, ])$data
ifelse (c == 0,
data_train_blc <- rbind(data_train_blc, train_blc),
data_train_blc <- rbind(data_train_blc, train_blc[train_blc$G3 == i, ]))
ifelse (c == 0,
data_test_blc <- rbind(data_test_blc, test_blc),
data_test_blc <- rbind(data_test_blc, test_blc[test_blc$G3 == i, ]))
c = c +1
}
round(prop.table(table(data_train_blc$G3)), 2)
round(prop.table(table(data_test_blc$G3)), 2)
round(prop.table(table(data_train$G3)), 2)
round(prop.table(table(data_train_blc$G3)), 2)
dim(data_train_blc)
dim(data_train)
round(prop.table(table(data_test$G3)), 2)
round(prop.table(table(data_test_blc$G3)), 2)
dim(data_test_blc)
dim(data_test)
# Verification of the class proportions and dimensions
round(prop.table(table(data_train$G3)), 2)
round(prop.table(table(data_train_blc$G3)), 2)
# Comparison of the class proportions and dimensions
round(prop.table(table(data_train$G3)), 2)
round(prop.table(table(data_train_blc$G3)), 2)
model_3 <- C5.0(G3 ~ ., data = data_train_blc, trials = 1)
model_3
# Summary
summary(model_3)
# Predictions with the test data set
predictions_3 <- predict(model_3, data_test)
# Confision Matrix
confusionMatrix(data_test$G3, predictions_3)
model_3 <- C5.0(G3 ~ G2, data = data_train_blc, trials = 1)
model_3
# Summary
summary(model_3)
# Predictions with the test data set
predictions_3 <- predict(model_3, data_test)
# Confision Matrix
confusionMatrix(data_test$G3, predictions_3)
# "F" is the biggest class, so the balancing will be done by it
data_train_blc <- data.frame()
data_test_blc <- data.frame()
c = 0
for (i in c("A","B","C","D")) {
train_blc <- ROSE(G3 ~ ., data = data_train[data_train$G3 == 'F' | data_train$G3 == i, ])$data
test_blc <- ROSE(G3 ~ ., data = data_test[data_test$G3 == 'F' | data_test$G3 == i, ])$data
ifelse (c == 0,
data_train_blc <- rbind(data_train_blc, train_blc),
data_train_blc <- rbind(data_train_blc, train_blc[train_blc$G3 == i, ]))
ifelse (c == 0,
data_test_blc <- rbind(data_test_blc, test_blc),
data_test_blc <- rbind(data_test_blc, test_blc[test_blc$G3 == i, ]))
c = c +1
}
# Comparison with the previous and balancing classes
round(prop.table(table(data_train$G3)), 2)
round(prop.table(table(data_train_blc$G3)), 2)
dim(data_train_blc)
dim(data_train)
round(prop.table(table(data_test$G3)), 2)
round(prop.table(table(data_test_blc$G3)), 2)
dim(data_test_blc)
dim(data_test)
model_3 <- C5.0(G3 ~ ., data = data_train_blc, trials = 1)
model_3
# Summary
summary(model_3)
# Predictions with the test data set
predictions_3 <- predict(model_3, data_test_blc)
# Confision Matrix
confusionMatrix(data_tes_blct$G3, predictions_3)
# Confision Matrix
confusionMatrix(data_tes_blc$G3, predictions_3)
# Confision Matrix
confusionMatrix(data_test_blc$G3, predictions_3)
predictions_3 <- predict(model_3, data_test)
confusionMatrix(data_test$G3, predictions_3)
model_3 <- C5.0(G3 ~ ., data = data_train_blc, trials = 1)
model_3
# Summary
summary(model_3)
# Predictions with the test data set
predictions_3 <- predict(model_3, data_test_blc)
predictions_31 <- predict(model_3, data_test)
# Confision Matrix
confusionMatrix(data_test_blc$G3, predictions_3)
confusionMatrix(data_test$G3, predictions_31)
confusionMatrix(data_test$G3, predictions_31)
# Confision Matrix
confusionMatrix(data_test_blc$G3, predictions_3)
confusionMatrix(data_test$G3, predictions_31)
# Sampling the data sets
data_train_blc
# Sampling the data sets
View(data_train_blc)
sample(c(1:nrow(data_train_blc)), size = nrow(data_train_blc), replace = F)
dim(data_train_blc)
sp_tr <- sample(c(1:nrow(data_train_blc)), size = nrow(data_train_blc), replace = F)
data_train_blc[sp_tr, ]
dim(data_train_blc[sp_tr, ])
data_train_blc <- data_train_blc[sp_tr, ]
head(data_train_blc)
head(data_train_blc)
sp_te <- sample(c(1:nrow(data_test_blc)), size = nrow(data_test_blc), replace = F)
data_test_blc <- data_test_blc[sp_te, ]
head(data_test_blc)
dim(data_test_blc)
model_3 <- C5.0(G3 ~ ., data = data_train_blc, trials = 1)
model_3
# Summary
summary(model_3)
# Predictions with the test data set
predictions_3 <- predict(model_3, data_test_blc)
predictions_31 <- predict(model_3, data_test)
# Confision Matrix
confusionMatrix(data_test_blc$G3, predictions_3)
confusionMatrix(data_test$G3, predictions_31)
# Configurando o diretório de trabalho
# Coloque entre aspas o diretório de trabalho que você está usando no seu computador
setwd("~/Documents/DSA/Big_Data_Analytics_com_R_e_Microsoft_Azure_Machine_Learning/Cap11_Machine_Learning_em_Linguagem_R/Classificacao")
getwd()
# Os dados do câncer da mama incluem 569 observações de biópsias de câncer,
# cada um com 32 características (variáveis). Uma característica é um número de
# identificação (ID), outro é o diagnóstico de câncer, e 30 são medidas laboratoriais
# numéricas. O diagnóstico é codificado como "M" para indicar maligno ou "B" para
# indicar benigno.
dados <- read.csv("dataset.csv", stringsAsFactors = FALSE)
# Excluindo a coluna ID
# Independentemente do método de aprendizagem de máquina, deve sempre ser excluídas
# variáveis de ID. Caso contrário, isso pode levar a resultados errados porque o ID
# pode ser usado para unicamente "prever" cada exemplo. Por conseguinte, um modelo
# que inclui um identificador pode sofrer de superajuste (overfitting),
# e será muito difícil usá-lo para generalizar outros dados.
dados$id <- NULL
# Ajustando o label da variável alvo
dados$diagnosis <- sapply(dados$diagnosis, function(x){ifelse(x=='M', 'Maligno', 'Benigno')})
# Muitos classificadores requerem que as variáveis sejam do tipo Fator
table(dados$diagnosis)
dados$diagnosis <- factor(dados$diagnosis, levels = c('Benigno', 'Maligno'), labels = c('Benigno', 'Maligno'))
str(dados)
# Verificando a proporção
round(prop.table(table(dados$diagnosis)) * 100, digits = 1)
# Medidas de Tendência Central
# Detectamos um problema de escala entre os dados, que então precisam ser normalizados
# O cálculo de distância feito pelo kNN é dependente das medidas de escala nos dados de entrada.
summary(dados[c('radius_mean', 'area_mean', 'smoothness_mean')])
# Criando um função de normalização
normalizar <- function(x){
return( (x - min(x)) / (max(x) - min(x)))
}
# Testando a função de normalização - os resultados devem ser idênticos
normalizar(c(1, 2, 3, 4, 5))
normalizar(c(10, 20, 30, 40, 50))
# Normalizando os dados
dados_norm <- as.data.frame((lapply(dados[2:31], normalizar)))
# Carregando o pacote library
# install.packages("class")
library(class)
?knn
# Criando dados de treino e dados de teste
dados_treino <- dados_norm[1:469, ]
dados_teste <- dados_norm[470:569, ]
# Criando os labels para os dados de treino e de teste
dados_treino_labels <- dados[1:469, 1]
dados_teste_labels <- dados[470:569, 1]
length(dados_treino_labels)
length(dados_teste_labels)
# Criando o modelo
modelo_knn_v1 <- knn(train = dados_treino,
test = dados_teste,
cl = dados_treino_labels,
k = 21) # 21 pontos mais próximos do ponto testado
# A função knn() retorna um objeto do tipo fator com as previsões para cada exemplo no dataset de teste
summary(modelo_knn_v1)
# Carregando o gmodels
# install.packages('gmodels')
library(gmodels)
# Criando uma tabela cruzada dos dados previstos x dados atuais
# Usaremos amostra com 100 observações: length(dados_teste_labels)
# Confusion Matrix
CrossTable(x = dados_teste_labels, y = modelo_knn_v1, prop.chisq = FALSE)
class(dados_treino_labels)
setwd("~/Documents/learning/R_scripts")
getwd()
# Packages
library(class)
# Read data set
df <- read.csv2('estudantes.csv', stringsAsFactors = T)
# Verifying if NA data exist
sum(is.na(df))
# Relative proportion of schools
round(prop.table(table(df$school)), 2)
# Sort by school
df <- df %>%
arrange(school)
View(df)
# Sort by school
df <- df %>%
arrange(school)
library(dplyr)
# Sort by school
df <- df %>%
arrange(school)
# ID column insertion
df$ID <- c(1:dim(df)[1])
# School IDs
GP_school <- df$ID[df$school == 'GP']
MS_school <- df$ID[df$school == 'MS']
# GP school
GP_school_test <- sample(GP_school, round(length(GP_school)*0.30))
GP_school_test <- df[df$ID %in% GP_school_test, ]
GP_school_test
GP_school_train <- GP_school[!(GP_school) %in% GP_school_test$ID]
GP_school_train <- df[df$ID %in% GP_school_train, ]
GP_school_train
# Checking if the number of rows of training and test data are equal to the total
(nrow(GP_school_test) + nrow(GP_school_train)) == length(GP_school)
# MS school
MS_school_test <- sample(MS_school, round(length(MS_school)*0.30))
MS_school_test <- df[df$ID %in% MS_school_test, ]
MS_school_test
MS_school_train <- MS_school[!(MS_school) %in% MS_school_test$ID]
MS_school_train <- df[df$ID %in% MS_school_train, ]
MS_school_train
# Checking if the number of rows of training and test data are equal to the total
(nrow(MS_school_test) + nrow(MS_school_train)) == length(MS_school)
# Concatenating the training data sets of the schools
data_train <- rbind(GP_school_train, MS_school_train)
data_train <- data_train[ , colnames(df) != 'ID']
dim(data_train)
# Concatenating the test data sets of the schools
data_test <- rbind(GP_school_test, MS_school_test)
data_test <- data_test[ , colnames(df) != 'ID']
dim(data_test)
### Making classes of grades, as in the reference of the data set site entitled
### "Cortes, P., Silva, A. USING DATA MINING TO PREDICT SECONDARY SCHOOL STUDENT
###  PERFORMANC. University of Minho. Portugal"
grade_classes <- function(x) {
if (x > 15) {x = "A"}
else if (x < 10) {x = "F"}
else if (x == 14 | x == 15) {x = "B"}
else if (x == 12 | x == 13) {x = "C"}
else {x = "D"}
}
data_train$G3 <- sapply(data_train$G3, grade_classes)
data_train$G3 <- as.factor(data_train$G3)
str(data_train$G3)
View(data_train)
dados_teste_labels <- dados[470:569, 1]
data_test$G3 <- sapply(data_test$G3, grade_classes)
data_test$G3 <- as.factor(data_test$G3)
str(data_test$G3)
data_train_labels <- as.factor(data_train$G3)
data_train_labels
data_test_labels <- as.factor(data_test$G3)
data_test_labels
# Model 1 - Decision Tree with G3 depending on all variables
#           in the training data set with any modification
modelo_1 <- knn(train = data_train,
test = data_test,
cl = data_train_labels,
k = 3)
model_1
# Model 1 - Decision Tree with G3 depending on all variables
#           in the training data set with any modification
modelo_1 <- knn(train = data_train,
test = data_test,
cl = data_train_labels,
k = 3)
data_train
data_train[c('G1', 'G2')]
# Model 1 - Decision Tree with G3 depending on all variables
#           in the training data set with any modification
modelo_1 <- knn(train = data_train[c('G1', 'G2')],
test = data_test[c('G1', 'G2')],
cl = data_train_labels,
k = 3)
model_1
# Model 1 - Decision Tree with G3 depending on all variables
#           in the training data set with any modification
model_1 <- knn(train = data_train[c('G1', 'G2')],
test = data_test[c('G1', 'G2')],
cl = data_train_labels,
k = 3)
model_1
# Summary
summary(model_1)
library(gmodels)
# Confision Matrix
CrossTable(x = dados_teste_labels, y = modelo_knn_v2, prop.chisq = FALSE)
# Confision Matrix
CrossTable(x = data_test_labels, y = model_1, prop.chisq = FALSE)
# Confision Matrix
CrossTable(x = data_test_labels, y = model_1, prop.chisq = TRUE)
# Summary
summary(model_1)
model_1
dim(model_1)
# The summary shows the predictions
summary(data_test_labels)
# The summary shows the predictions
summary(data_test_labels)[1]
model_1[0]
# Confision Matrix
CrossTable(x = data_test_labels, y = model_1, prop.chisq = FALSE)
# Predictions with the test data set
predictions_1 <- predict(model_1, data_test)
# Predictions with the test data set
predictions_1 <- predict(model_1, data_test_labels)
confusionMatrix(data_test$G3, predictions_1)
?confusionMatrix
??confusionMatrix
library(caret)
confusionMatrix(data_test$G3, predictions_1)
confusionMatrix(data_test$G3, model_1)
# Confusion Matrix
confusionMatrix(data_test$G3, model_1)
# Model 1 - KNN
str(data_train)
setwd("~/Documents/learning/R_scripts")
# Packages
library(class)
library(dplyr)
library(gmodels)
library(caret)
?knn
# Read data set
df <- read.csv2('estudantes.csv', stringsAsFactors = F)
# Verifying if NA data exist
sum(is.na(df))
# Relative proportion of schools
round(prop.table(table(df$school)), 2)
# Sort by school
df <- df %>%
arrange(school)
# ID column insertion
df$ID <- c(1:dim(df)[1])
# School IDs
GP_school <- df$ID[df$school == 'GP']
MS_school <- df$ID[df$school == 'MS']
# GP school
GP_school_test <- sample(GP_school, round(length(GP_school)*0.30))
GP_school_test <- df[df$ID %in% GP_school_test, ]
GP_school_test
GP_school_train <- GP_school[!(GP_school) %in% GP_school_test$ID]
GP_school_train <- df[df$ID %in% GP_school_train, ]
GP_school_train
# Checking if the number of rows of training and test data are equal to the total
(nrow(GP_school_test) + nrow(GP_school_train)) == length(GP_school)
# MS school
MS_school_test <- sample(MS_school, round(length(MS_school)*0.30))
MS_school_test <- df[df$ID %in% MS_school_test, ]
MS_school_test
MS_school_train <- MS_school[!(MS_school) %in% MS_school_test$ID]
MS_school_train <- df[df$ID %in% MS_school_train, ]
MS_school_train
# Checking if the number of rows of training and test data are equal to the total
(nrow(MS_school_test) + nrow(MS_school_train)) == length(MS_school)
# Concatenating the training data sets of the schools
data_train <- rbind(GP_school_train, MS_school_train)
data_train <- data_train[ , colnames(df) != 'ID']
dim(data_train)
# Concatenating the test data sets of the schools
data_test <- rbind(GP_school_test, MS_school_test)
data_test <- data_test[ , colnames(df) != 'ID']
dim(data_test)
### Making classes of grades, as in the reference of the data set site entitled
### "Cortes, P., Silva, A. USING DATA MINING TO PREDICT SECONDARY SCHOOL STUDENT
###  PERFORMANC. University of Minho. Portugal"
grade_classes <- function(x) {
if (x > 15) {x = "A"}
else if (x < 10) {x = "F"}
else if (x == 14 | x == 15) {x = "B"}
else if (x == 12 | x == 13) {x = "C"}
else {x = "D"}
}
data_train$G3 <- sapply(data_train$G3, grade_classes)
data_train$G3 <- as.factor(data_train$G3)
data_test$G3 <- sapply(data_test$G3, grade_classes)
data_test$G3 <- as.factor(data_test$G3)
data_train_labels <- as.factor(data_train$G3)
data_train_labels
data_test_labels <- as.factor(data_test$G3)
data_test_labels
# Model 1 - KNN
str(data_train)
model_1 <- knn(train = data_train,
test = data_test,
cl = data_train_labels,
k = 3)
model_1 <- knn(train = data_train,
test = data_test,
cl = data_train_labels,
k = 3)
# Model 1 - KNN
str(data_train)
typeof[data_train]
# Model 1 - KNN
str(data_train)
# Model 1 - KNN
str(data_train) == integer()
# Model 1 - KNN
str(data_train) == integer()
# Model 1 - KNN
is.integer(str(data_train))
select_if(data_train, is.numeric)
# Model 1 - KNN
str(data_train)
select_if(data_train, is.numeric)
# The KNN algorithm does not except string or factor type
# Selecting the integer columns
str(data_train)
data_train_int <- select_if(data_train, is.numeric)
head(data_train_int)
# The KNN algorithm does not except string or factor type
# Selecting the numeric columns
str(data_train)
data_train_int <- select_if(data_train, is.numeric)
head(data_train_int)
data_test_int <- select_if(data_test, is.numeric)
head(data_test_int)
data_train_num <- select_if(data_train, is.numeric)
head(data_train_num)
data_test_num <- select_if(data_test, is.numeric)
head(data_test_num)
# Model 1 - KNN
model_1 <- knn(train = data_train_num,
test = data_test_num,
cl = data_train_labels,
k = 3)
model_1
# Summary
summary(model_1)
# Confusion Matrix
confusionMatrix(data_test$G3, model_1)
# Other Confusion Matrix
CrossTable(x = data_test_labels, y = model_1, prop.chisq = FALSE)
